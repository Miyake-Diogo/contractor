{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is designed to process text files in a structured manner. It begins by loading the necessary data and defining agents and tools required for the task. These agents and tools are configured to work sequentially, ensuring that the text files located in the root directory are properly loaded. Once the text files are loaded, the script processes their content by resuming (extracting key points) from each file. Finally, it generates a concise summary of all the texts, providing a high-level overview of the information contained in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding src folder to path: c:\\Users\\ricar\\Github\\augumented-rag\\src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "print(\"Adding src folder to path:\", src_path)\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "load_dotenv(f\"{src_path}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models from the Source Folder\n",
    "\n",
    "To begin, ensure that the required models are loaded from the source folder. This step is crucial for initializing the necessary components for processing and analysis. Verify that the source folder contains all the relevant model files before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.schemas.models import VideoData, AudioData, ImageData, TextData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Models\n",
    "\n",
    "In the next cell, we will load the data models corresponding to various data sources: audio, text, video, and image. These models represent the structured data from the respective folders inside the `./data` directory. Each model is designed to handle the specific characteristics of its data type, ensuring efficient processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure ACA and its features' tags=['azure', 'aca', 'containers'] encoding=None embeddings=None\n",
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure in general' tags=['azure', 'definitions'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "audio_data_1 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure ACA and its features\",\n",
    "    tags=[\"azure\", \"aca\", \"containers\"]\n",
    ")\n",
    "\n",
    "audio_data_2 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure in general\",\n",
    "    tags=[\"azure\", \"definitions\"]\n",
    ")\n",
    "\n",
    "print(\"AudioData instance:\", audio_data_1)\n",
    "print(\"AudioData instance:\", audio_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextData instance: source='./data/documents/aoai-assistants.pdf' objective='Explains Azure OpenAI and its features' tags=['azure', 'aoai', 'assistants'] encoding=None embeddings=None\n",
      "TextData instance: source='./data/documents/aoai-prompting.pdf' objective='Explains Prompting in Azure OpenAI' tags=['azure', 'aoai', 'prompting'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "text_data_1 = TextData(\n",
    "    source=\"./data/documents/aoai-assistants.pdf\",\n",
    "    objective=\"Explains Azure OpenAI and its features\",\n",
    "    tags=[\"azure\", \"aoai\", \"assistants\"]\n",
    ")\n",
    "\n",
    "text_data_2 = TextData(\n",
    "    source=\"./data/documents/aoai-prompting.pdf\",\n",
    "    objective=\"Explains Prompting in Azure OpenAI\",\n",
    "    tags=[\"azure\", \"aoai\", \"prompting\"]\n",
    ")\n",
    "\n",
    "print(\"TextData instance:\", text_data_1)\n",
    "print(\"TextData instance:\", text_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Toolers and Creating the Assembly\n",
    "\n",
    "In this step, we focus on loading the appropriate toolers required for processing the data. Toolers are specialized components designed to handle specific tasks, such as extracting embeddings, encoding data, or performing analysis. Once the toolers are loaded, they are assembled into a cohesive workflow to ensure seamless processing of the data models (audio, video, image, and text).\n",
    "\n",
    "The assembly of toolers ensures that each data type is processed using the most suitable tools, leveraging their unique capabilities to extract meaningful insights and achieve the desired objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly created with agents: id='8868e737-56b2-4991-bd3e-943de429d3df' objective='multimodal processing using local data for architecture review on azure' agents=[Agent(id='6a286f12-6309-4ca4-a764-b5543f5ce625', name='AudioAgent', model_id='default', metaprompt='This agent handles audio processing and extraction tasks.', objective='audio'), Agent(id='f9192c13-d801-4cb7-8354-97d91147a6d9', name='TextAgent', model_id='default', metaprompt='This agent analyzes textual information for semantic understanding.', objective='text')] roles=['audio', 'text']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from app.schemas.models import Agent, Assembly\n",
    "\n",
    "audio_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"AudioAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent handles audio processing and extraction tasks.\",\n",
    "    objective=\"audio\"\n",
    ")\n",
    "\n",
    "text_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"TextAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent analyzes textual information for semantic understanding.\",\n",
    "    objective=\"text\"\n",
    ")\n",
    "\n",
    "assembly = Assembly(\n",
    "    id=str(uuid.uuid4()),\n",
    "    objective=\"multimodal processing using local data for architecture review on azure\",\n",
    "    agents=[audio_agent, text_agent],\n",
    "    roles=[\"audio\", \"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Assembly created with agents:\", assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.main import ToolerOrchestrator\n",
    "\n",
    "orchestrator = ToolerOrchestrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Erro durante a transcrição de áudio: Exception with error code: \n",
      "[CALL STACK BEGIN]\n",
      "\n",
      "    > pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - recognizer_create_speech_recognizer_from_config\n",
      "\n",
      "[CALL STACK END]\n",
      "\n",
      "Exception with an error code: 0x8 (SPXERR_FILE_OPEN_FAILED)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: It seems you've sent an empty prompt. Could you please provide more details or let me know how I can assist you?\n",
      "I encountered issues while processing the documents and audios:\n",
      "\n",
      "1. **Text Processing**: The summarization service is currently not configured, so I couldn't generate abstracts for the documents (`aoai-assistants.pdf`, `aoai-prompting.pdf`, `aoai.pdf`).\n",
      "\n",
      "2. **Audio Processing**: There was an error with the audio transcription due to a file access problem (`SPXERR_FILE_OPEN_FAILED`). Therefore, I couldn't extract details from the audio files.\n",
      "\n",
      "If you can reconfigure the services or provide alternate methods, please let me know so I can proceed effectively. Additionally, I could manually edit, review, or process any content based on further instructions.\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(\n",
    "    assembly=assembly,\n",
    "    prompt=\"\"\"\n",
    "        You have been given a few local documents and audios on the folder located at the directory 'C:\\\\Users\\\\ricar\\\\Github\\\\augumented-rag\\\\notebook\\\\data'\n",
    "        and contains the subfolders 'documents' and 'audios'. I need a abstract detailing their content.\n",
    "        Preciselly give the name of the Python Classes used as Tools on your answer.\n",
    "    \"\"\",\n",
    "    strategy=\"parallel\"\n",
    ")\n",
    "\n",
    "flattened = [\n",
    "    str(item)\n",
    "    for sublist in response\n",
    "    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "]\n",
    "\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: Tools available:\n",
      "• Text extraction and analysis tool (to extract content from Microsoft Azure documentation and related references).\n",
      "\n",
      "Using Reference: Microsoft Azure documentation on Azure OpenAI, AKS, and ACA\n",
      "\n",
      "Answer:\n",
      "You can build a chatbot that leverages Azure OpenAI models on container orchestration platforms like Azure Kubernetes Service (AKS) or Azure Container Apps (ACA) by following these general steps:\n",
      "\n",
      "1. Provision the Azure OpenAI Service:\n",
      "  • Create an Azure OpenAI resource in your subscription via the Azure portal.\n",
      "  • Retrieve your API endpoint and key. These credentials are required so your chatbot code can authenticate and interact with the chosen model (for example, ChatGPT or another language model).\n",
      "\n",
      "2. Develop Your Chatbot Application:\n",
      "  • Write your chatbot code (in Python, Node.js, etc.) so that it calls the Azure OpenAI endpoint. Your application should format user inputs into API requests and then process the responses.\n",
      "  • Implement state management, conversation handling, and any additional business logic or integration points (e.g., session management, context, etc.).\n",
      "\n",
      "3. Containerize Your Application:\n",
      "  • Create a Dockerfile to define your application’s runtime configuration.\n",
      "  • Build your Docker image and test it locally.\n",
      "  • Push the Docker image to a container registry like Azure Container Registry (ACR).\n",
      "\n",
      "4. Deploying on AKS versus ACA:\n",
      "  A. Using Azure Kubernetes Service (AKS):\n",
      "   • Provision an AKS cluster either via the Azure portal, Azure CLI, or ARM templates.\n",
      "   • Deploy your containerized chatbot using Kubernetes manifests:\n",
      "    – Create a Deployment manifest that specifies your Docker image, replicas, and environment variables (such as your OPENAI_API_KEY, which you can reference from Kubernetes Secrets).\n",
      "    – Expose your application using a Service (ClusterIP or LoadBalancer) and optionally configure an Ingress Controller for routing.\n",
      "   • AKS offers full control over cluster configuration, scaling, and networking.\n",
      "\n",
      "  B. Using Azure Container Apps (ACA):\n",
      "   • ACA is a fully managed, serverless container environment that abstracts away much of the underlying infrastructure management.\n",
      "   • Use the Azure CLI or portal to create a new Container App:\n",
      "    – Specify your image from your ACR.\n",
      "    – Define the target port, environment variables, and scaling rules.\n",
      "    – Configure ingress settings so that your chatbot is accessible externally.\n",
      "   • ACA simplifies deployment, stress handling, and scaling without managing the container orchestration infrastructure directly.\n",
      "\n",
      "5. Security and Configuration:\n",
      "  • Store sensitive configuration items (like your API keys) securely—either using Kubernetes Secrets (for AKS) or ACA environment variables with secure vault integration.\n",
      "  • Optionally, use Azure Key Vault to centrally manage secrets and use managed identities for secure access.\n",
      "\n",
      "6. Monitoring and Maintenance:\n",
      "  • Use Azure Monitor, Application Insights, or other logging tools to track your chatbot’s performance and health.\n",
      "  • Set up continuous integration/continuous deployment (CI/CD) pipelines to seamlessly update and maintain your container images and deployments.\n",
      "\n",
      "In summary, you integrate Azure OpenAI into your chatbot application by coding the logic to interact with the OpenAI endpoints, containerizing your solution, and then deploying it to either an AKS cluster—for full control—or ACA—for a fully managed, serverless environment. Your choice depends on the level of management and control you need.\n",
      "\n",
      "Reference:\n",
      "- Microsoft Azure Documentation on Azure OpenAI, AKS, and ACA (using our text extraction tool to extract relevant guidelines).\n",
      "\n",
      "This approach ensures that you can quickly set up a scalable, secure chatbot leveraging state-of-the-art AI models in the containerized environment suiting your operational requirements.\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Explain how may I use Azure OpenAI to build a chatbot on AKS or ACA\", strategy=\"llm\")\n",
    "flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Detail each information that I have to consider on Azure Container Instance to Run a multi-agentic RAG app\", strategy=\"llm\")\n",
    "#flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "#print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
